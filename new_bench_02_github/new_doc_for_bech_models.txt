# ì„±ëŠ¥ë¹„êµ Baseline ëª¨ë¸ 10ê°œ - ë…¼ë¬¸ë³„ ì„±ëŠ¥ & êµ¬í˜„ ìƒì„¸

## **1. WLS (Weighted Least Squares)**

### ğŸ“Š ë…¼ë¬¸ ìƒ ì„±ëŠ¥
| ë…¼ë¬¸ | í™˜ê²½ | WLS ì„±ëŠ¥ | ë¹„êµ ëŒ€ìƒ | ë¹„ê³  |
|------|------|----------|-----------|------|
| #20 | ì‹¤ë‚´ í˜¼í•© LOS/NLOS | MAE ~0.5-0.7m (ì¶”ì •) | Proposed(0.289m)ë³´ë‹¤ ë†’ìŒ | Baselineìœ¼ë¡œ ì‚¬ìš© |
| #28 | ì‹¤ë‚´ | ìœ„ì¹˜ì˜¤ì°¨ **0.250m** (ë³´ì • ì „) | Proposed(0.091m) | NLOS ë³´ì • ì „ |
| #50 | ê±´ì„¤í˜„ì¥ | ì •í™•ë„ ë‚®ìŒ (ìˆ˜ì¹˜ ë¯¸ëª…ì‹œ) | EKF ëŒ€ë¹„ ì—´ë“± | ì •ì  í™˜ê²½ ê°€ì • |

### ğŸ”§ êµ¬í˜„ ë°©ë²•
```python
# ë…¼ë¬¸ #20ì—ì„œ ì‚¬ìš©í•œ ë°©ì‹
def WLS(anchor_positions, ranges, weights=None):
    """
    Input:
    - anchor_positions: (N, 2) ì•µì»¤ ì¢Œí‘œ
    - ranges: (N,) ì¸¡ì • ê±°ë¦¬
    - weights: (N,) ê±°ë¦¬ë³„ ê°€ì¤‘ì¹˜ (ì¼ë°˜ì ìœ¼ë¡œ 1/sigma^2)
    
    Output:
    - position: (2,) ì¶”ì • ì¢Œí‘œ (x, y)
    """
    # 1. ì„ í˜•í™”: (x-ai)^2 + (y-bi)^2 = ri^2
    # 2. ìµœì†ŒììŠ¹: A @ [x,y] = b í˜•íƒœë¡œ ë³€í™˜
    # 3. ê°€ì¤‘ì¹˜ ì ìš©: W @ A @ [x,y] = W @ b
    # 4. í•´: position = inv(A.T @ W @ A) @ A.T @ W @ b
    
    # ì¼ë°˜ì ìœ¼ë¡œ ì²« ì•µì»¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì°¨ë¶„ ë°©ì •ì‹ êµ¬ì„±
    # weightsê°€ Noneì´ë©´ ëª¨ë‘ 1 (LSì™€ ë™ì¼)
```

**í•µì‹¬ íŒŒë¼ë¯¸í„°:**
- ê°€ì¤‘ì¹˜ ì„ íƒ: `1/distance^2` ë˜ëŠ” `1/variance` 
- ì´ˆê¸°í™”: í•„ìš” ì—†ìŒ (closed-form)
- ìµœì†Œ ì•µì»¤: 3ê°œ (2D), 4ê°œ (3D)

---

## **2. EKF (Extended Kalman Filter)**

### ğŸ“Š ë…¼ë¬¸ ìƒ ì„±ëŠ¥
| ë…¼ë¬¸ | í™˜ê²½ | EKF ì„±ëŠ¥ | ìƒíƒœ ë²¡í„° | ë¹„ê³  |
|------|------|----------|-----------|------|
| #1 | ì‹¤ë‚´ LOS/NLOS | RMSE ê°œì„  (ìˆ˜ì¹˜ ë¯¸ëª…ì‹œ) | [x, y, z, vx, vy, vz] | LOS/NLOS í™•ë¥  ê°€ì¤‘ |
| #12 | ì‹¤ë‚´ ë³µí•© | **61.12% ê°œì„ ** (ë³´ì • í›„) | [x, y, Î¸, v] (INS ìœµí•©) | VBGMM + GPDAF ì „ì²˜ë¦¬ |
| #50 | ê±´ì„¤í˜„ì¥ | **RMSE â‰¤0.5m** (3ì•µì»¤) | [x, y] | ì •ì  + kinematic |
| #76 | ì˜¤í”¼ìŠ¤ | RMSE ~0.3-0.4m (ì¶”ì •) | [x, y, vx, vy] | DEKF(ë¶„ì‚°) ì‚¬ìš© ì „ |

### ğŸ”§ êµ¬í˜„ ë°©ë²• (#1, #12 ê¸°ë°˜)
```python
# ìƒíƒœ ë²¡í„°: [x, y, vx, vy]
# ì¸¡ì •: UWB ê±°ë¦¬

class EKF_UWB:
    def __init__(self):
        # ìƒíƒœ: [x, y, vx, vy]
        self.x = np.zeros(4)
        
        # ê³µë¶„ì‚°
        self.P = np.eye(4) * 10
        
        # í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ (ë…¼ë¬¸ #1 ê¸°ì¤€)
        self.Q = np.diag([0.1, 0.1, 0.5, 0.5])  # ìœ„ì¹˜/ì†ë„ ë…¸ì´ì¦ˆ
        
        # ì¸¡ì • ë…¸ì´ì¦ˆ (LOS/NLOS ì ì‘)
        self.R_LOS = 0.1**2      # LOS: sigma = 10cm
        self.R_NLOS = 0.5**2     # NLOS: sigma = 50cm
    
    def predict(self, dt):
        # ìƒíƒœ ì „ì´: constant velocity model
        F = np.array([
            [1, 0, dt, 0],
            [0, 1, 0, dt],
            [0, 0, 1, 0],
            [0, 0, 0, 1]
        ])
        self.x = F @ self.x
        self.P = F @ self.P @ F.T + self.Q
    
    def update(self, anchor_pos, measured_range, nlos_prob=0.0):
        # ì¸¡ì • ëª¨ë¸: h(x) = sqrt((x-ax)^2 + (y-ay)^2)
        dx = self.x[0] - anchor_pos[0]
        dy = self.x[1] - anchor_pos[1]
        predicted_range = np.sqrt(dx**2 + dy**2)
        
        # Jacobian
        H = np.array([[dx/predicted_range, dy/predicted_range, 0, 0]])
        
        # ì¸¡ì • ë…¸ì´ì¦ˆ ì ì‘ (#1ì˜ LOS/NLOS í™•ë¥  ê°€ì¤‘)
        R = self.R_LOS * (1 - nlos_prob) + self.R_NLOS * nlos_prob
        
        # Kalman gain
        S = H @ self.P @ H.T + R
        K = self.P @ H.T / S
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        innovation = measured_range - predicted_range
        self.x = self.x + K.flatten() * innovation
        self.P = (np.eye(4) - K @ H) @ self.P
```

**í•µì‹¬ íŒŒë¼ë¯¸í„° (#1, #12ì—ì„œ ì‚¬ìš©):**
- í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ: `Q_pos = 0.1`, `Q_vel = 0.5`
- ì¸¡ì • ë…¸ì´ì¦ˆ: `R_LOS = 0.01`, `R_NLOS = 0.25`
- dt: `0.1s` (10Hz ì¸¡ì • ê°€ì •)
- ì´ˆê¸° ê³µë¶„ì‚°: `P0 = diag([10, 10, 1, 1])`

---

## **3. SVM (Support Vector Machine)**

### ğŸ“Š ë…¼ë¬¸ ìƒ ì„±ëŠ¥
| ë…¼ë¬¸ | íƒœìŠ¤í¬ | SVM ì„±ëŠ¥ | ì»¤ë„/íŒŒë¼ë¯¸í„° | ë¹„ê³  |
|------|------|----------|---------------|------|
| #28 | **LOS/NLOS ë¶„ë¥˜** | **Accuracy 93.27%** | RBF, DTC+FC-SVM | 2ë‹¨ê³„ ë¶„ë¥˜ |
| #28 | **ìœ„ì¹˜ ì¶”ì •** | **MAE 0.091m** (ë³´ì • í›„) | - | ë¶„ë¥˜ í›„ WLS |
| #55 | NLOS ë¶„ë¥˜ (ë¹„êµêµ°) | 78-82% | LS-SVM | F-BERT(95%+) ëŒ€ë¹„ ë‚®ìŒ |

### ğŸ”§ êµ¬í˜„ ë°©ë²• (#28 ìƒì„¸)

#### **1ë‹¨ê³„: DTC (Dynamic Threshold Classification)**
```python
# CIRì—ì„œ íŠ¹ì§• ì¶”ì¶œ (CCPs)
def extract_CCPs(cir):
    """
    CIR Characteristic Parameters (CCPs) ì¶”ì¶œ
    """
    # ENS/FP/MP 3ë‹¨ê³„ ë¶„ì„
    L = S * NTM  # LDE threshold
    
    # 1. FCN (False Crests Number)
    fcn = count_peaks_before_first_path(cir, L)
    
    # 2. FPE (FP Error)
    fp_detected = detect_first_path(cir, L)
    fpe = abs(fp_detected - true_fp_index)
    
    # 3. FDE (FP Distance Error)  
    fde = fpe * sampling_time * speed_of_light
    
    # ê¸°íƒ€ í†µê³„ íŠ¹ì§•
    mean_cir = np.mean(cir)
    std_cir = np.std(cir)
    max_cir = np.max(cir)
    
    return [fcn, fpe, fde, mean_cir, std_cir, max_cir, ...]

# DTC: ë™ì  ì„ê³„ë¡œ 1ì°¨ ë¶„ë¥˜
def DTC_classify(features, threshold_los=0.3, threshold_nlos=0.7):
    # Simple rule-based 1st stage
    # ìƒì„¸ ê·œì¹™ì€ ë…¼ë¬¸ ìˆ˜ì‹ ì°¸ê³ 
    if features['fpe'] < threshold_los:
        return 'LOS'
    elif features['fpe'] > threshold_nlos:
        return 'NLOS'
    else:
        return 'Uncertain'  # FC-SVMìœ¼ë¡œ ë„˜ê¹€
```

#### **2ë‹¨ê³„: FC-SVM (Fuzzy Credibility SVM)**
```python
from sklearn.svm import SVC

# Training
svm = SVC(kernel='rbf', C=10.0, gamma='scale', probability=True)

# í•™ìŠµ ë°ì´í„°: DTCì—ì„œ 'Uncertain'ìœ¼ë¡œ ë¶„ë¥˜ëœ ìƒ˜í”Œë“¤
X_train = extract_CCPs(cir_uncertain)  # shape: (N, n_features)
y_train = labels_uncertain  # 0: LOS, 1: NLOS

svm.fit(X_train, y_train)

# Fuzzy Credibility: ë¶„ë¥˜ í™•ë¥ ì— fuzzy membership ì ìš©
def fuzzy_credibility(svm_prob, fc_alpha=0.8):
    # fc_alpha: credibility factor
    fuzzy_prob = svm_prob ** fc_alpha
    return fuzzy_prob / fuzzy_prob.sum()

# Inference
cir_test = get_cir()
features = extract_CCPs(cir_test)

# 1ë‹¨ê³„
dtc_result = DTC_classify(features)
if dtc_result != 'Uncertain':
    prediction = dtc_result
else:
    # 2ë‹¨ê³„
    svm_prob = svm.predict_proba([features])[0]
    fuzzy_prob = fuzzy_credibility(svm_prob)
    prediction = 'LOS' if fuzzy_prob[0] > 0.5 else 'NLOS'
```

**í•µì‹¬ íŒŒë¼ë¯¸í„° (#28):**
- CCPs íŠ¹ì§•: FCN, FPE, FDE + 7ê°œ í†µê³„
- SVM ì»¤ë„: RBF
- C: 10.0
- gamma: 'scale'
- FC alpha: 0.8
- DTC ì„ê³„: LOS=0.3, NLOS=0.7

---

## **4. Random Forest**

### ğŸ“Š ë…¼ë¬¸ ìƒ ì„±ëŠ¥
| ë…¼ë¬¸ | íƒœìŠ¤í¬ | RF ì„±ëŠ¥ | íŒŒë¼ë¯¸í„° | ë¹„ê³  |
|------|------|---------|----------|------|
| #10 | ê±°ë¦¬ íšŒê·€ (BLE) | MAE 20-30cm | n_estimators=100 | Per-beacon ëª¨ë¸ |
| (ì¼ë°˜) | NLOS ë¶„ë¥˜ | 85-90% (ì¶”ì •) | - | ë§ì€ ë…¼ë¬¸ì—ì„œ ë¹„êµêµ° |

### ğŸ”§ êµ¬í˜„ ë°©ë²• (ì¼ë°˜ì )
```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

# ë¶„ë¥˜ìš© (LOS/NLOS)
rf_classifier = RandomForestClassifier(
    n_estimators=100,        # íŠ¸ë¦¬ ê°œìˆ˜
    max_depth=10,            # ìµœëŒ€ ê¹Šì´
    min_samples_split=10,    # ë¶„í•  ìµœì†Œ ìƒ˜í”Œ
    min_samples_leaf=5,      # ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ
    max_features='sqrt',     # íŠ¹ì§• ìƒ˜í”Œë§
    random_state=42
)

# ìœ„ì¹˜ íšŒê·€ìš©
rf_regressor = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    random_state=42
)

# í•™ìŠµ
X_train = extract_features(cir_train)  # CIR íŠ¹ì§•
y_train = labels_train

rf_classifier.fit(X_train, y_train)

# ì¶”ë¡ 
prediction = rf_classifier.predict(X_test)
```

**ê¶Œì¥ íŒŒë¼ë¯¸í„°:**
- n_estimators: 100-200
- max_depth: 10-15
- min_samples_split: 5-10
- íŠ¹ì§•: CIR í†µê³„ (í‰ê· , í‘œì¤€í¸ì°¨, ì™œë„, ì²¨ë„, ì—ë„ˆì§€ ë“±)

---

## **5. k-NN (k-Nearest Neighbors)**

### ğŸ“Š ë…¼ë¬¸ ìƒ ì„±ëŠ¥
| ë…¼ë¬¸ | íƒœìŠ¤í¬ | k-NN ì„±ëŠ¥ | k ê°’ | ë¹„ê³  |
|------|------|-----------|------|------|
| (ì¼ë°˜) | NLOS ë¶„ë¥˜ | 80-85% | k=5-7 | ë‹¨ìˆœ baseline |
| (ì¼ë°˜) | Fingerprinting | MAE 0.5-1m | k=3-5 | ë°ì´í„° ì˜ì¡´ì  |

### ğŸ”§ êµ¬í˜„ ë°©ë²•
```python
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor

# ë¶„ë¥˜ìš©
knn_classifier = KNeighborsClassifier(
    n_neighbors=5,
    weights='distance',  # ê±°ë¦¬ ê°€ì¤‘ (ë˜ëŠ” 'uniform')
    metric='euclidean',
    algorithm='auto'
)

# íšŒê·€ìš© (ì¢Œí‘œ ì§ì ‘ ì˜ˆì¸¡)
knn_regressor = KNeighborsRegressor(
    n_neighbors=3,
    weights='distance',
    metric='euclidean'
)

# í•™ìŠµ (ë‹¨ìˆœ ì €ì¥)
knn_classifier.fit(X_train, y_train)

# ì¶”ë¡ 
prediction = knn_classifier.predict(X_test)
```

**ê¶Œì¥ íŒŒë¼ë¯¸í„°:**
- k: 3-7 (ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¼)
- weights: 'distance' (ê°€ê¹Œìš´ ì´ì›ƒì— ë” í° ê°€ì¤‘ì¹˜)
- metric: 'euclidean' ë˜ëŠ” 'manhattan'

---

## **6. CNN (Convolutional Neural Network)**

### ğŸ“Š ë…¼ë¬¸ ìƒ ì„±ëŠ¥
| ë…¼ë¬¸ | íƒœìŠ¤í¬ | CNN ì„±ëŠ¥ | êµ¬ì¡° | ë¹„ê³  |
|------|------|----------|------|------|
| #53 | **NLOS ë¶„ë¥˜** | **85-90%** | 1D CNN (PDP ì…ë ¥) | ê²½ëŸ‰í™” |
| #72 | **Fingerprinting** | **MAE 20.9-87.0cm** | 1D CNN (CIR ì…ë ¥) | ì„œë¸Œì…‹ ì•™ìƒë¸” |
| #78 | **ì˜¤ì°¨ íšŒê·€** | **MAE 0.1004m** | ECA-ResNet | Greenhouse |

### ğŸ”§ êµ¬í˜„ ë°©ë²• (#72 ê¸°ë°˜)

```python
import torch
import torch.nn as nn

class CNN_UWB_Classifier(nn.Module):
    """
    #72 ë…¼ë¬¸ì˜ CNN Fingerprinting ê¸°ë°˜
    """
    def __init__(self, cir_length=1016, num_classes=3):
        super().__init__()
        
        # Conv blocks
        self.conv1 = nn.Sequential(
            nn.Conv1d(1, 32, kernel_size=5, padding=2),
            nn.ReLU(),
            nn.MaxPool1d(2)  # 1016 -> 508
        )
        
        self.conv2 = nn.Sequential(
            nn.Conv1d(32, 64, kernel_size=5, padding=2),
            nn.ReLU(),
            nn.MaxPool1d(2)  # 508 -> 254
        )
        
        self.conv3 = nn.Sequential(
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2)  # 254 -> 127
        )
        
        # Fully connected
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 127, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        # x: (batch, 1, cir_length)
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.fc(x)
        return x

# í•™ìŠµ ì„¤ì • (#72, #53 ê¸°ë°˜)
model = CNN_UWB_Classifier()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# í•™ìŠµ ë£¨í”„
for epoch in range(500):  # #53: 500 epochs
    optimizer.zero_grad()
    outputs = model(cir_batch)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
```

**í•µì‹¬ íŒŒë¼ë¯¸í„° (#53, #72):**
- ì…ë ¥: CIR 1D (1016 samples) ë˜ëŠ” PDP
- Conv layers: 3-4ê°œ
- Kernel size: 3-5
- Channels: 32 â†’ 64 â†’ 128
- Optimizer: Adam, lr=1e-3
- Epochs: 500
- Batch size: 256 (#53)

---

## **7. LSTM / CNN-LSTM**

### ğŸ“Š ë…¼ë¬¸ ìƒ ì„±ëŠ¥
| ë…¼ë¬¸ | íƒœìŠ¤í¬ | ì„±ëŠ¥ | êµ¬ì¡° | ë¹„ê³  |
|------|------|------|------|------|
| #76 | **UWB/IMU ìœ„ì¹˜** | **RMSE 0.205m, MAE 0.192m** | CNN-LSTM + DEKF | Office í™˜ê²½ |
| #55 | NLOS ë¶„ë¥˜ (ë¹„êµêµ°) | 80-85% | CNN-LSTM | F-BERT(95%+) ëŒ€ë¹„ ë‚®ìŒ |

### ğŸ”§ êµ¬í˜„ ë°©ë²• (#76 ìƒì„¸)

```python
import torch
import torch.nn as nn

class CNN_LSTM_UWB(nn.Module):
    """
    #76 ë…¼ë¬¸ì˜ CNN-LSTM êµ¬ì¡°
    CIR â†’ CNN (íŠ¹ì§• ì¶”ì¶œ) â†’ LSTM (ì‹œê³„ì—´) â†’ ë¶„ë¥˜/íšŒê·€
    """
    def __init__(self, cir_length=1016, lstm_hidden=128, num_classes=3):
        super().__init__()
        
        # CNN feature extractor
        self.cnn = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=7, padding=3),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(2),  # 1016 -> 508
            
            nn.Conv1d(64, 128, kernel_size=5, padding=2),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.MaxPool1d(2),  # 508 -> 254
            
            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(32)  # 254 -> 32 (ê³ ì • ê¸¸ì´)
        )
        
        # LSTM for temporal modeling
        self.lstm = nn.LSTM(
            input_size=256,
            hidden_size=lstm_hidden,
            num_layers=2,
            batch_first=True,
            dropout=0.3,
            bidirectional=True  # #76ì—ì„œ ì‚¬ìš©
        )
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(lstm_hidden * 2, 128),  # *2 for bidirectional
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x):
        # x: (batch, seq_len, cir_length)
        batch_size, seq_len, cir_len = x.shape
        
        # CNN: process each timestep
        x = x.view(batch_size * seq_len, 1, cir_len)
        cnn_out = self.cnn(x)  # (batch*seq, 256, 32)
        cnn_out = cnn_out.permute(0, 2, 1)  # (batch*seq, 32, 256)
        cnn_out = cnn_out.reshape(batch_size, seq_len, 32, 256)
        cnn_out = cnn_out.mean(dim=2)  # (batch, seq, 256)
        
        # LSTM: temporal modeling
        lstm_out, _ = self.lstm(cnn_out)  # (batch, seq, hidden*2)
        
        # Take last timestep
        last_out = lstm_out[:, -1, :]  # (batch, hidden*2)
        
        # Classification
        output = self.classifier(last_out)
        return output

# í•™ìŠµ ì„¤ì • (#76 ê¸°ë°˜)
model = CNN_LSTM_UWB()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

# í•™ìŠµ ë£¨í”„
for epoch in range(150):  # #76: 150 epochs ì¶”ì •
    # ... training code ...
    scheduler.step()
```

**í•µì‹¬ íŒŒë¼ë¯¸í„° (#76):**
- ì…ë ¥: CIR ì‹œí€€ìŠ¤ (seq_len=10-20)
- CNN: 3 layers, channels [64, 128, 256]
- LSTM: 2 layers, hidden=128, bidirectional
- Optimizer: Adam, lr=5e-4
- Scheduler: StepLR (30 epochë§ˆë‹¤ /10)
- Dropout: 0.3 (LSTM), 0.5 (FC)
- Epochs: 100-150

---

## **8. TCN-Attention (Temporal Convolutional Network + Self-Attention)**

### ğŸ“Š ë…¼ë¬¸ ìƒ ì„±ëŠ¥
| ë…¼ë¬¸ | íƒœìŠ¤í¬ | ì„±ëŠ¥ | êµ¬ì¡° | ë¹„ê³  |
|------|------|------|------|------|
| #22 | **NLOS ë¶„ë¥˜** (Channel 1) | **90.49% (Mixed)** | CNN-TCN-Attention | ì´ì¤‘ì±„ë„ |
| #22 | **ìœ„ì¹˜ ì¶”ì •** | **X: 404mm, Y: 239mm** | + TLS | ë³´ì • í›„ |

### ğŸ”§ êµ¬í˜„ ë°©ë²• (#22 ìƒì„¸)

```python
import torch
import torch.nn as nn

class TCN_Block(nn.Module):
    """
    Temporal Convolutional Network Block
    Causal + Dilated convolution
    """
    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):
        super().__init__()
        padding = (kernel_size - 1) * dilation
        
        self.conv = nn.Conv1d(
            in_channels, out_channels,
            kernel_size=kernel_size,
            padding=padding,
            dilation=dilation
        )
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        
        # Residual connection
        self.downsample = nn.Conv1d(in_channels, out_channels, 1) \
            if in_channels != out_channels else None
    
    def forward(self, x):
        out = self.conv(x)
        out = out[:, :, :x.size(2)]  # Causal: remove future
        out = self.relu(out)
        out = self.dropout(out)
        
        res = x if self.downsample is None else self.downsample(x)
        return out + res

class SelfAttention(nn.Module):
    """Self-Attention mechanism"""
    def __init__(self, embed_dim):
        super().__init__()
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        self.scale = embed_dim ** 0.5
    
    def forward(self, x):
        # x: (batch, seq, embed)
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)
        
        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        attn_weights = torch.softmax(scores, dim=-1)
        
        # Weighted sum
        out = torch.matmul(attn_weights, V)
        return out, attn_weights

class CNN_TCN_Attention(nn.Module):
    """
    #22 ë…¼ë¬¸ì˜ Channel 1: CNN + TCN + Self-Attention
    """
    def __init__(self, cir_length=256, num_classes=2):  # 650-906ns êµ¬ê°„
        super().__init__()
        
        # Initial CNN (feature extraction)
        self.cnn = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2),  # 256 -> 128
            
            nn.Conv1d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2),  # 128 -> 64
            
            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2)  # 64 -> 16
        )
        
        # TCN stack (6 layers with increasing dilation)
        self.tcn = nn.Sequential(
            TCN_Block(128, 64, kernel_size=3, dilation=1),
            TCN_Block(64, 64, kernel_size=3, dilation=2),
            TCN_Block(64, 64, kernel_size=3, dilation=4),
            TCN_Block(64, 64, kernel_size=3, dilation=8),
            TCN_Block(64, 64, kernel_size=3, dilation=16),
            TCN_Block(64, 64, kernel_size=3, dilation=32)
        )
        
        # Self-Attention
        self.attention = SelfAttention(embed_dim=64)
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x):
        # x: (batch, 1, cir_length)
        
        # CNN feature extraction
        x = self.cnn(x)  # (batch, 128, 16)
        
        # TCN temporal modeling
        x = self.tcn(x)  # (batch, 64, 16)
        
        # Self-Attention
        x = x.permute(0, 2, 1)  # (batch, 16, 64)
        x, attn_weights = self.attention(x)
        x = x.permute(0, 2, 1)  # (batch, 64, 16)
        
        # Classification
        output = self.classifier(x)
        return output, attn_weights

# í•™ìŠµ ì„¤ì • (#22 ê¸°ë°˜)
model = CNN_TCN_Attention()
criterion = nn.BCEWithLogitsLoss()  # Binary: LOS/NLOS
optimizer = torch.optim.Nadam(model.parameters(), lr=1e-3)

# Plateau scheduler
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.1, patience=3
)

# í•™ìŠµ ë£¨í”„
for epoch in range(50):  # #22: 50 epochs
    # ... training ...
    scheduler.step(val_loss)
```

**í•µì‹¬ íŒŒë¼ë¯¸í„° (#22):**
- ì…ë ¥: CIR 650-906ns êµ¬ê°„ (256 samples)
- CNN: 3 layers [1â†’64â†’64â†’128]
- TCN: 6 layers, dilations [1,2,4,8,16,32]
- Attention: embed_dim=64
- Optimizer: Nadam, lr=1e-3
- Scheduler: ReduceLROnPlateau, patience=3
- Epochs: 50
- Batch size: 32

---

## **9. Transformer/BERT (Fuzzy Transformer)**

### ğŸ“Š ë…¼ë¬¸ ìƒ ì„±ëŠ¥
| ë…¼ë¬¸ | íƒœìŠ¤í¬ | ì„±ëŠ¥ | ë¹„êµ ëŒ€ìƒ | ê°œì„ í­ |
|------|------|------|-----------|--------|
| #55 | **NLOS ì‹ë³„** | **95%+** | LS-SVM(80%), CNN-LSTM(82%) | **+12.5-14.9%p** |
| #55 | **ê±°ë¦¬ ë³´ì •** | **NLOS ì˜¤ì°¨ 36.2%â†“** | - | - |
| #55 | **ìœ„ì¹˜ ì •í™•ë„** | **37.9%â†‘** | Baseline ëŒ€ë¹„ | - |

### ğŸ”§ êµ¬í˜„ ë°©ë²• (#55 ìƒì„¸)

```python
import torch
import torch.nn as nn
from transformers import BertModel, BertConfig

class FuzzyTransformer_UWB(nn.Module):
    """
    #55 ë…¼ë¬¸ì˜ F-BERT (Fuzzy BERT)
    CIR íŠ¹ì§• + Fuzzy membership â†’ BERT â†’ NLOS ë¶„ë¥˜
    """
    def __init__(self, num_features=20, bert_hidden=128, num_classes=2):
        super().__init__()
        
        # Feature embedding
        self.feature_embed = nn.Linear(num_features, bert_hidden)
        
        # Fuzzy membership layer
        self.fuzzy_layer = nn.Sequential(
            nn.Linear(bert_hidden, bert_hidden),
            nn.Sigmoid()  # Fuzzy membership [0,1]
        )
        
        # BERT encoder
        config = BertConfig(
            hidden_size=bert_hidden,
            num_hidden_layers=4,  # ê²½ëŸ‰í™”
            num_attention_heads=4,
            intermediate_size=bert_hidden * 4,
            max_position_embeddings=32
        )
        self.bert = BertModel(config)
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(bert_hidden, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, num_classes)
        )
    
    def extract_features(self, cir):
        """
        ìƒˆë¡œìš´ CIR íŠ¹ì§• ì¶”ì¶œ (FPE, ESP, FPN ë“±)
        #55 ë…¼ë¬¸ì˜ Table I ì°¸ê³ 
        """
        features = []
        
        # 1. FPE (First Path Error)
        fp_idx = detect_first_path(cir)
        fpe = compute_fpe(cir, fp_idx)
        features.append(fpe)
        
        # 2. ESP (Energy of Signal Path)
        esp = compute_energy(cir[fp_idx:fp_idx+50])
        features.append(esp)
        
        # 3. FPN (First Path Number)
        fpn = count_significant_peaks(cir[:fp_idx])
        features.append(fpn)
        
        # ... (ì´ 20ê°œ íŠ¹ì§•)
        # ë…¼ë¬¸ Table Iì— ë‚˜ì—´ëœ íŠ¹ì§•ë“¤
        
        return torch.tensor(features, dtype=torch.float32)
    
    def forward(self, cir_batch):
        # CIR â†’ Feature extraction
        batch_size = cir_batch.size(0)
        features = torch.stack([
            self.extract_features(cir) for cir in cir_batch
        ])  # (batch, num_features)
        
        # Feature embedding
        embedded = self.feature_embed(features)  # (batch, bert_hidden)
        
        # Fuzzy membership
        fuzzy_features = self.fuzzy_layer(embedded)  # (batch, bert_hidden)
        
        # Add sequence dimension for BERT
        seq_input = fuzzy_features.unsqueeze(1)  # (batch, 1, bert_hidden)
        
        # BERT encoding
        bert_out = self.bert(inputs_embeds=seq_input)
        pooled = bert_out.pooler_output  # (batch, bert_hidden)
        
        # Classification
        output = self.classifier(pooled)
        return output

# ë²”ìœ„ ë³´ì •ìš© BERT (2nd stage)
class RangeCorrection_BERT(nn.Module):
    """
    NLOSë¡œ ë¶„ë¥˜ëœ ê±°ë¦¬ë¥¼ ë³´ì •
    """
    def __init__(self, num_features=20, bert_hidden=128):
        super().__init__()
        # êµ¬ì¡°ëŠ” F-BERTì™€ ìœ ì‚¬í•˜ë‚˜ ì¶œë ¥ì´ íšŒê·€ê°’
        # ... (ìœ„ì™€ ìœ ì‚¬í•œ êµ¬ì¡°)
        self.regressor = nn.Linear(bert_hidden, 1)  # ê±°ë¦¬ ì˜¤ì°¨ ì˜ˆì¸¡
    
    def forward(self, features, measured_range):
        # ... BERT encoding ...
        delta_range = self.regressor(bert_out)
        corrected_range = measured_range + delta_range
        return corrected_range

# í•™ìŠµ ì„¤ì • (#55 ê¸°ë°˜)
f_bert = FuzzyTransformer_UWB()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(f_bert.parameters(), lr=2e-5, weight_decay=0.01)

# Warmup + Linear decay
from transformers import get_linear_schedule_with_warmup
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=500,
    num_training_steps=10000
)

# í•™ìŠµ ë£¨í”„
for epoch in range(20):  # #55: 20-30 epochs ì¶”ì •
    # ... training ...
    scheduler.step()
```

**í•µì‹¬ íŒŒë¼ë¯¸í„° (#55):**
- íŠ¹ì§•: 20ê°œ (FPE, ESP, FPN ë“± ìƒˆë¡œìš´ íŠ¹ì§•)
- BERT layers: 4 (ê²½ëŸ‰í™”)
- BERT hidden: 128
- Attention heads: 4
- Optimizer: AdamW, lr=2e-5, weight_decay=0.01
- Warmup: 500 steps
- Epochs: 20-30
- Fuzzy alpha: 0.8

---

## **10. ARPF (Adaptive Robust Particle Filter)**

### ğŸ“Š ë…¼ë¬¸ ìƒ ì„±ëŠ¥
| ë…¼ë¬¸ | í™˜ê²½ | ARPF ì„±ëŠ¥ | ë¹„êµ ëŒ€ìƒ | ë¹„ê³  |
|------|------|-----------|-----------|------|
| #18 | ì‹¤ë‚´ 16Ã—16m | **95% â‰¤1.2m** | GPF ëŒ€ë¹„ **ìƒëŒ€ì˜¤ì°¨ 1/2** | Helmet/Badge/Watch |
| #18 | TDOA (3ì•µì»¤) | **Max error <2.4m** (worst case) | - | Badge (worst) |

### ğŸ”§ êµ¬í˜„ ë°©ë²• (#18 ìƒì„¸)

```python
import numpy as np
from scipy.stats import chi2

class ARPF_UWB:
    """
    #18 ë…¼ë¬¸ì˜ Adaptive Robust Particle Filter
    Multi-level constraint: ì‹œê°„ì´ë ¥ + IGG-III + Floor-map
    """
    def __init__(self, num_particles=2000, anchor_positions=None, floor_map=None):
        self.N = num_particles
        self.anchors = anchor_positions
        self.floor_map = floor_map  # 0/1 binary map (accessible/not)
        
        # Particles
        self.particles = np.random.uniform([0, 0], [10, 10], (self.N, 2))
        self.weights = np.ones(self.N) / self.N
        
        # IGG-III parameters
        self.c1 = 2.5  # First boundary
        self.c2 = 6.0  # Second boundary
        
        # Time history gate
        self.v_human = 1.5  # m/s (human walking speed)
        self.dt = 0.1  # s
        
        self.prev_tdoa = None
    
    def time_history_gate(self, tdoa_measurement):
        """
        ì‹œê°„ì´ë ¥ ê²Œì´íŒ…: ê¸‰ê²©í•œ ë³€í™” ë°°ì œ
        """
        if self.prev_tdoa is None:
            self.prev_tdoa = tdoa_measurement
            return np.ones(len(tdoa_measurement), dtype=bool)
        
        delta_tdoa = np.abs(tdoa_measurement - self.prev_tdoa)
        max_delta = self.v_human * self.dt / 3e8 * 1e9  # Convert to time
        
        valid_mask = delta_tdoa < max_delta
        self.prev_tdoa = tdoa_measurement
        return valid_mask
    
    def igg3_robust_weight(self, residual, sigma):
        """
        IGG-III robust weighting
        """
        v_normalized = np.abs(residual) / sigma
        
        if v_normalized <= self.c1:
            return 1.0
        elif v_normalized <= self.c2:
            weight = self.c1 / v_normalized * \
                     ((self.c2 - v_normalized) / (self.c2 - self.c1)) ** 2
            return weight
        else:
            return 0.0  # Outlier: ë°°ì œ
    
    def floor_map_constraint(self, particle):
        """
        Floor-map ì ‘ê·¼ì„± ì²´í¬
        """
        x, y = particle
        # Convert to grid coordinates
        grid_x = int(x / 0.1)  # 10cm grid
        grid_y = int(y / 0.1)
        
        if self.floor_map[grid_y, grid_x] == 0:
            return 0.0  # Not accessible
        else:
            return 1.0
    
    def predict(self, dt):
        """
        Prediction step: random walk
        """
        process_noise = np.random.normal(0, 0.2, (self.N, 2)) * dt
        self.particles += process_noise
        
        # Boundary check
        self.particles = np.clip(self.particles, [0, 0], [10, 10])
    
    def update(self, tdoa_measurements, measurement_sigma):
        """
        Update step with multi-level robust weighting
        """
        # 1. Time history gate
        valid_mask = self.time_history_gate(tdoa_measurements)
        
        for i in range(self.N):
            particle = self.particles[i]
            
            # Compute expected TDOA
            expected_tdoa = []
            for anchor in self.anchors:
                dist = np.linalg.norm(particle - anchor)
                expected_tdoa.append(dist / 3e8 * 1e9)  # Convert to ns
            expected_tdoa = np.array(expected_tdoa)
            
            # Residuals
            residuals = tdoa_measurements - expected_tdoa
            
            # 2. IGG-III robust weighting
            R_scales = []
            for j, residual in enumerate(residuals):
                if not valid_mask[j]:
                    R_scales.append(np.inf)  # Exclude
                else:
                    weight = self.igg3_robust_weight(residual, measurement_sigma[j])
                    if weight == 0:
                        R_scales.append(np.inf)
                    else:
                        R_scales.append(1.0 / weight)
            R_scales = np.array(R_scales)
            
            # 3. Floor-map constraint
            map_weight = self.floor_map_constraint(particle)
            if map_weight == 0:
                R_scales = np.inf * np.ones_like(R_scales)
            
            # Compute weighted likelihood
            R_scaled = measurement_sigma ** 2 * R_scales
            log_likelihood = -0.5 * np.sum(residuals ** 2 / R_scaled)
            self.weights[i] = np.exp(log_likelihood)
        
        # Normalize weights
        self.weights += 1e-300  # Avoid zero
        self.weights /= np.sum(self.weights)
        
        # Effective sample size
        N_eff = 1.0 / np.sum(self.weights ** 2)
        
        # Resampling if needed
        if N_eff < self.N / 3:
            self.resample()
    
    def resample(self):
        """
        Systematic resampling
        """
        indices = np.random.choice(self.N, self.N, p=self.weights)
        self.particles = self.particles[indices]
        self.weights = np.ones(self.N) / self.N
    
    def estimate(self):
        """
        Position estimate: weighted mean
        """
        return np.average(self.particles, weights=self.weights, axis=0)

# ì‚¬ìš© ì˜ˆì‹œ
arpf = ARPF_UWB(
    num_particles=2000,
    anchor_positions=anchor_coords,
    floor_map=accessibility_map
)

# Tracking loop
for tdoa, sigma in zip(tdoa_sequence, sigma_sequence):
    arpf.predict(dt=0.1)
    arpf.update(tdoa, sigma)
    position = arpf.estimate()
```

**í•µì‹¬ íŒŒë¼ë¯¸í„° (#18):**
- ì…ì ìˆ˜: **N = 2000**
- IGG-III: c1=2.5, c2=6.0
- ì†ë„ ì œí•œ: v_max = 1.5 m/s
- í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ: sigma = 0.2 m
- ë¦¬ìƒ˜í”Œ ì„ê³„: N_eff < N/3
- ê²©ì í¬ê¸°: 10cm (floor-map)

---

## **ğŸ“‹ ì„±ëŠ¥ ìš”ì•½ ë¹„êµí‘œ**

| ëª¨ë¸ | ëŒ€í‘œ ë…¼ë¬¸ | ì£¼ìš” íƒœìŠ¤í¬ | ìµœê³  ì„±ëŠ¥ | í•™ìŠµ ì‹œê°„ | ì¶”ë¡  ì†ë„ | êµ¬í˜„ ë‚œì´ë„ |
|------|----------|------------|----------|----------|----------|------------|
| **WLS** | #20, #28 | ìœ„ì¹˜ | MAE 0.25-0.7m | - | ë§¤ìš° ë¹ ë¦„ | â­ |
| **EKF** | #1, #12, #50 | ìœ„ì¹˜ | RMSE 0.3-0.5m | - | ë¹ ë¦„ | â­â­ |
| **SVM** | #28 | ë¶„ë¥˜+ìœ„ì¹˜ | **Acc 93.27%, MAE 0.091m** | ë¹ ë¦„ | ë¹ ë¦„ | â­â­ |
| **Random Forest** | #10 | ë¶„ë¥˜/íšŒê·€ | MAE 0.2-0.3m | ë³´í†µ | ë¹ ë¦„ | â­ |
| **k-NN** | (ì¼ë°˜) | ë¶„ë¥˜/íšŒê·€ | Acc 80-85% | ì—†ìŒ | ëŠë¦¼ | â­ |
| **CNN** | #53, #72, #78 | ë¶„ë¥˜/íšŒê·€ | **MAE 0.10-0.21m** | ë³´í†µ | ë¹ ë¦„ | â­â­â­ |
| **LSTM/CNN-LSTM** | #76 | ìœ„ì¹˜ | **RMSE 0.205m, MAE 0.192m** | ëŠë¦¼ | ë³´í†µ | â­â­â­â­ |
| **TCN-Attention** | #22 | ë¶„ë¥˜+ìœ„ì¹˜ | **Acc 90%+, X:404mm, Y:239mm** | ëŠë¦¼ | ë³´í†µ | â­â­â­â­â­ |
| **Transformer** | #55 | ë¶„ë¥˜+ë³´ì • | **Acc 95%+, ìœ„ì¹˜ 37.9%â†‘** | ë§¤ìš° ëŠë¦¼ | ëŠë¦¼ | â­â­â­â­â­ |
| **ARPF** | #18 | ìœ„ì¹˜ | **95% â‰¤1.2m** | - | ëŠë¦¼ | â­â­â­â­ |



ë…¼ë¬¸ ë²ˆí˜¸ì— ë§ëŠ” ì„¤ëª… ë§í¬


ttps://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/20.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/28.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/1.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/12.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/50.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/28.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/10.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/53.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/72.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/78.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/76.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/22.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/55.txt
https://raw.githubusercontent.com/IECAD/paperfor/refs/heads/main/ref84/18.txt
